{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the Equator Evaluator! This notebook is designed to test state-of-the-art language models (LLMs) either locally or via API. We’ve chosen to use OpenRouter because it’s OpenAI-compatible and provides access to over 276 different models.\n",
    "\n",
    "In addition, we can evaluate local Ollama-based models. With a bit of effort, you can adapt any model—local or remote—that follows the OpenAI API format. Keep in mind that evaluations on local models may run more slowly than those on remote API models, owing to your machine’s memory constraints. Although we run the Equator Evaluator locally, you can also host it on a remote server.\n",
    "\n",
    "For our evaluations, we’ll use the OpenAI API to access OpenRouter models. We’ve tested the free models to ensure they work as expected. Remember, local evaluations may still be slower than using an external API. Our official model evaluations will be presented on our website. Meanwhile, we’ll maintain a private list of over 1,005 reasoning and logic questions to guarantee that our results remain unbiased.\n",
    "\n",
    "Our tool is versatile enough to handle any QA evaluations—including legal, medical, or financial—by simply adding them to the *linguistic_benchmark.json* file. Our project focuses on identifying logical and reasoning shortcomings in LLMs to help strengthen their problem-solving abilities. We’ve found that LLMs can be easily tricked, so our goal is to track their progress until they truly match human-level capabilities.\n",
    "\n",
    "Looking ahead, our next step is to incorporate vision into the Equator Evaluator. We’re also planning to release more advanced, locally-runnable reasoning models soon.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started, please follow these steps:\n",
    "\n",
    "1. **Obtain Your OpenRouter Key**  \n",
    "   Visit https://openrouter.ai/settings/keys to get your OpenRouter key.\n",
    "\n",
    "2. **Add Funds to Your Account**  \n",
    "   Make sure to add a few dollars to your account so you can use any of the models they provide. For more information, visit https://openrouter.ai/models.\n",
    "\n",
    "3. **Create a .env File**  \n",
    "   In your root directory, create a .env file with the following line:  \n",
    "   ```\n",
    "   OPENROUTER_KEY=\"<add your API key from OpenRouter>\"\n",
    "   ```\n",
    "\n",
    "4. **Install Ollama Locally**  \n",
    "   Since we will be using LLaMa 3.2 3b as our evaluator, please install Ollama locally. Note that this model can be changed, but if you do so, you will need to edit the line in the `auto_eval_bernard_llm_vector_db_remote_qa.py` file at line 385:  \n",
    "   ```python\n",
    "   response = self.generate_chat(\n",
    "       model=\"llama3.2\", messages=evaluator_system_prompt, stream=False\n",
    "   )\n",
    "   ```\n",
    "\n",
    "5. **Download Ollama**  \n",
    "   You can download Ollama from https://ollama.com/.\n",
    "\n",
    "6. **Pull the LLaMa Model**  \n",
    "   Run the following command to pull the latest LLaMa model:  \n",
    "   ```bash\n",
    "   ollama pull llama3.2:latest\n",
    "   ```\n",
    "\n",
    "7. **Run Ollama**  \n",
    "   Finally, execute Ollama with the command:  \n",
    "   ```bash\n",
    "   ollama run llama3.2\n",
    "   ``` \n",
    "\n",
    "Make sure to follow each step carefully to ensure everything is set up correctly!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Make sure you create a new python virtual environment and activate it! Run the below cell once!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports just need to run it to but not an issue if you run it multiple times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext dotenv\n",
    "%dotenv\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import requests\n",
    "import chromadb\n",
    "import time \n",
    "from loguru import logger\n",
    "\n",
    "from charting import create_performance_chart\n",
    "from utils import get_llm_stats, load_all_llm_answers_from_json\n",
    "from openai import OpenAI\n",
    "\n",
    "# import csv\n",
    "import sqlite3\n",
    "from datetime import datetime  # Correct import\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from auto_eval_bernard import Bernard_Controller, VectorDB_Controller, extract_model_parts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Instructions :  Variables\n",
    "This section allows us to configure various configurations of the LLM Evaluator. For example if you just want to run the static analysis just comment out the llm_evaluate in the execution list.  You can also set the models you like to evaluate.  We are using OpenAI api call to the openrouter_models.  Change them to the models you like to evaluate.   Open router has about 275 models to choose.  They also have free models which are limited to about 200 calls per day. So you will need to create a paid account and use the none free models to avoid the limitation.  We have evaluated the free models just to test the code and make sure everything works as expected.\n",
    "\n",
    "\n",
    "\n",
    "With respect to keepVectorDB you can set it to true to avoid imputing the data if you have already done it.  Please note that we input the data from linguistic_bechmark.json.  You are free to customize it for your purposes.  This data is the source of truth for our evaluator.  It is the answer key for grading the  \"student\".  \n",
    "\n",
    "Also with respect to folder directory structures,  you can hard code the date which will keep using the same directory structure.   This section allows you to configure various settings for the LLM Evaluator. For instance, if you only want to run the static analysis, simply comment out the `llm_evaluate` in the execution list. You can also specify the models you wish to evaluate. We use the OpenAI API to access the openrouter models, and you can change them to any models of your choice. OpenRouter offers about 275 models, including free options limited to approximately 200 calls per day. To avoid this limitation, you will need to create a paid account to access the non-free models. We have evaluated the free models to test the code and ensure everything works as expected.\n",
    "\n",
    "Regarding the `keepVectorDB` setting, you can set it to true to prevent re-inputting data if you have already done so. Please note that we input the data from `linguistic_benchmark.json`. Feel free to customize this file for your purposes, as it serves as the source of truth for our evaluator and acts as the answer key for grading the \"student.\"\n",
    "\n",
    "Additionally, concerning folder directory structures, you can hard-code the date to maintain a consistent directory structure.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "execution_steps = [\n",
    "        \"llm_evaluate\",\n",
    "        \"generate_statistics\",\n",
    "    ]\n",
    "\n",
    "local_student = \"llm\"\n",
    "openrouter_models = [\"google/learnlm-1.5-pro-experimental:free\",\"meta-llama/llama-3.2-11b-vision-instruct:free\",\n",
    "                        \"nousresearch/hermes-3-llama-3.1-405b:free\",\"qwen/qwen-2-7b-instruct:free\",\"microsoft/phi-3-medium-128k-instruct:free\"]\n",
    "answer_rounds = 2 # Number of rounds of questions to ask each model\n",
    "benchmark_name = \"Bernard\"\n",
    "# Change to false if you want a new vector db\n",
    "keepVectorDB = False\n",
    "# date_now=\"2024-11-30\"  # datetime.now().strftime('%Y-%m-%d')\n",
    "date_now = datetime.now().strftime('%Y-%m-%d')\n",
    "folder_name = f\"{date_now}-{benchmark_name}\"\n",
    "\n",
    "auto_eval_save_path = f\"./{folder_name}/auto_eval_outputs\"\n",
    "stats_save_path = f\"./{folder_name}/tables_and_charts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in openrouter_models:\n",
    "    model_path = model\n",
    "    lab, student_models = extract_model_parts(model)\n",
    "    if student_models:\n",
    "        print(f\"Extracted Lab name: {lab}\")\n",
    "\n",
    "        print(f\"Extracted model name: {student_models}\")\n",
    "    else:\n",
    "        print(\"Model name not found.\")\n",
    "    student_models = [student_models]\n",
    "\n",
    "    VectorDB_Controller(keepVectorDB)\n",
    "\n",
    "\n",
    "    if \"llm_evaluate\" in execution_steps:\n",
    "        print(\"1. GETTING BERNARD LLM Evaluator ANSWERS\")\n",
    "        for n in range(answer_rounds):\n",
    "            print(f\"\\n----- Round: {n+1} of {answer_rounds} -----\")\n",
    "            answer_save_path_round = f\"{auto_eval_save_path}\"\n",
    "\n",
    "            Bernard_Controller(\n",
    "                model_path,\n",
    "                lab,\n",
    "                student_models,\n",
    "                answer_save_path_round=answer_save_path_round,\n",
    "                count=n,\n",
    "                prefix_replace=\"auto_eval-\",\n",
    "            )\n",
    "\n",
    "if \"generate_statistics\" in execution_steps:\n",
    "    sub_eval_folders = [f\"/round_{r+1}\" for r in range(answer_rounds)]\n",
    "\n",
    "    print(\"2. GENERATING STATISTICS\")\n",
    "    all_stats_dfs = {}\n",
    "    save_info = [\n",
    "        {\n",
    "            \"path\": auto_eval_save_path,\n",
    "            \"chart_title\": \"LLM Linguistic Benchmark Performance\",\n",
    "            \"type\": \"\",\n",
    "        }\n",
    "    ]\n",
    "    for info in save_info:\n",
    "        save_path = info[\"path\"]\n",
    "        chart_title = info[\"chart_title\"]\n",
    "        info_type = info[\"type\"]\n",
    "        print(\"Eval for path:\", save_path)\n",
    "        all_llm_evals = load_all_llm_answers_from_json(\n",
    "            save_path,\n",
    "            prefix_replace=\"auto_eval-\",\n",
    "            sub_folders=sub_eval_folders,\n",
    "        )\n",
    "        stats_df = get_llm_stats(\n",
    "            all_llm_evals, stats_save_path, file_suffix=info_type, bootstrap_n=10000\n",
    "        )\n",
    "\n",
    "        display(stats_df)\n",
    "\n",
    "        barplot, plt = create_performance_chart(\n",
    "            stats_df.reset_index(),\n",
    "            chart_title,\n",
    "            highlight_models=[\"o1-preview\"],\n",
    "        )\n",
    "        barplot.figure.savefig(f\"{stats_save_path}/performance_chart{info_type}.png\")\n",
    "        plt.show()\n",
    "        all_stats_dfs[chart_title] = stats_df\n",
    "\n",
    "    print(\"-- DONE STATS --\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional Charts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Read data from CSV file\n",
    "df = pd.read_csv(f'{stats_save_path}\\\\final_stats.csv')\n",
    "\n",
    "# Sorting DataFrame by Mean Score in descending order for better visualization\n",
    "df_sorted = df.sort_values(by='mean_score', ascending=False)\n",
    "\n",
    "# Color palette from the provided PDF\n",
    "colors = {\n",
    "    'blue_200': '#90caf9',\n",
    "    'yellow_600': '#fdd835',\n",
    "    'pink_200': '#f48fb1',\n",
    "    'cyan_200': '#80deea',\n",
    "    'orange_400': '#ffa726',\n",
    "    'deep_purple_A100': '#b388ff',\n",
    "    'red_700': '#d32f2f'\n",
    "}\n",
    "\n",
    "# Horizontal Bar Chart for Mean Score, CI Lower, and CI Upper for Each Model (Sorted in Descending Order)\n",
    "y = np.arange(len(df_sorted['model']))  # the label locations\n",
    "height = 0.25  # the height of the bars\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "bars1 = ax.barh(y - height, df_sorted['mean_score'], height, label='Mean Score', color=colors['blue_200'])\n",
    "bars2 = ax.barh(y, df_sorted['ci_lower'], height, label='CI Lower', color=colors['yellow_600'])\n",
    "bars3 = ax.barh(y + height, df_sorted['ci_upper'], height, label='CI Upper', color=colors['cyan_200'])\n",
    "\n",
    "# Adding labels and title\n",
    "ax.set_yticks(y)\n",
    "ax.set_yticklabels(df_sorted['model'])  # Labels on the left\n",
    "ax.set_xlabel('Scores')\n",
    "ax.set_title('Comparison of Mean Score, CI Lower, and CI Upper for Each Model')\n",
    "ax.invert_yaxis()  # Higher values at the top\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.5)\n",
    "plt.show()\n",
    "\n",
    "# Horizontal Bar Chart for Z Interval Error for Each Model (Sorted in Descending Order)\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "bars = ax.barh(df_sorted['model'], df_sorted['z_interval_error'], color=colors['pink_200'])\n",
    "\n",
    "plt.ylabel('Models')\n",
    "plt.xlabel('Z Interval Error')\n",
    "plt.title('Z Interval Error for Each Model')\n",
    "ax.invert_yaxis()  # Higher values at the top\n",
    "plt.tight_layout()\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.5)\n",
    "plt.show()\n",
    "\n",
    "# Horizontal Bar Chart for Mean Score of Each Model (Sorted in Descending Order)\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "bars = ax.barh(df_sorted['model'], df_sorted['mean_score'], color=colors['orange_400'])\n",
    "\n",
    "plt.ylabel('Models')\n",
    "plt.xlabel('Mean Score')\n",
    "plt.title('Mean Score for Each Model')\n",
    "ax.invert_yaxis()  # Higher values at the top\n",
    "plt.tight_layout()\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.5)\n",
    "plt.show()\n",
    "\n",
    "# Plotting Mean Score with Error Bars for Confidence Intervals (Sorted in Descending Order)\n",
    "ci_error = (df_sorted['ci_upper'] - df_sorted['ci_lower']).abs() / 2\n",
    "plt.figure(figsize=(14, 10))\n",
    "plt.errorbar(df_sorted['mean_score'], df_sorted['model'], \n",
    "             xerr=ci_error, \n",
    "             fmt='o', ecolor=colors['red_700'], capsize=5, label='Mean Score with CI')\n",
    "plt.ylabel('Models')\n",
    "plt.xlabel('Mean Score')\n",
    "plt.title('Mean Score with Confidence Intervals for Various Models')\n",
    "plt.gca().invert_yaxis()  # Higher values at the top\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.5)\n",
    "plt.show()\n",
    "\n",
    "# Bar Chart of Standard Deviations for Each Model\n",
    "\n",
    "# Create a bar chart where each model is represented individually\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Plotting standard deviation scores for each model\n",
    "ax.bar(df['model'], df['std_dev_score'], color='#90caf9', edgecolor='black')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xticks(rotation=45, ha='right', fontsize=8)\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Standard Deviation')\n",
    "plt.title('Standard Deviation for Each Model')\n",
    "plt.tight_layout()\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Analysis\n",
    "This provides a straightforward measure of the tokens used per category across all models in a specific run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# Base directory containing rounds (e.g., 'auto_eval_save_path')\n",
    "auto_eval_save_path = auto_eval_save_path\n",
    "\n",
    "# Directory to save the output files\n",
    "charts_dir = stats_save_path\n",
    "\n",
    "os.makedirs(charts_dir, exist_ok=True)\n",
    "\n",
    "# Number of rounds\n",
    "answer_rounds = 2  # Update as needed\n",
    "\n",
    "# Function to collect all JSON file paths in a directory\n",
    "def collect_json_files(directory):\n",
    "    return [os.path.join(directory, file) for file in os.listdir(directory) if file.endswith('.json')]\n",
    "\n",
    "# Function to process JSON files\n",
    "def process_json_files(file_paths):\n",
    "    results = []\n",
    "    for file_path in file_paths:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            data = json.load(file)\n",
    "        for _, entry in data.items():\n",
    "            # Token calculation for each category\n",
    "            question_tokens = count_tokens(entry.get(\"question\", \"\"))\n",
    "            human_answer_tokens = count_tokens(entry.get(\"human_answer\", \"\"))\n",
    "            model_answer_input_tokens = count_tokens(entry.get(\"model_answer\", \"\"))\n",
    "            eval_response_tokens = count_tokens(entry.get(\"eval_response\", \"\"))\n",
    "            score_tokens = count_tokens(str(entry.get(\"score\", \"\")))\n",
    "            bernard_evaluator_response_tokens = count_tokens(entry.get(\"bernard_evaluator_response\", \"\"))\n",
    "            \n",
    "            results.append({\n",
    "                \"question_tokens\": question_tokens,\n",
    "                \"human_answer_tokens\": human_answer_tokens,\n",
    "                \"model_answer_input_tokens\": model_answer_input_tokens,\n",
    "                \"eval_response_tokens\": eval_response_tokens,\n",
    "                \"score_tokens\": score_tokens,\n",
    "                \"bernard_evaluator_response_tokens\": bernard_evaluator_response_tokens,\n",
    "                \"total_tokens\": question_tokens + human_answer_tokens + model_answer_input_tokens +\n",
    "                                eval_response_tokens + score_tokens + bernard_evaluator_response_tokens\n",
    "            })\n",
    "    return results\n",
    "\n",
    "# Function to calculate tokens based on the rule: 1 token = 4 characters\n",
    "def count_tokens(text):\n",
    "    return max(1, len(text) // 4)\n",
    "\n",
    "# Process files for each round\n",
    "all_results = []\n",
    "\n",
    "for round_num in range(1, answer_rounds + 1):\n",
    "    round_dir = os.path.join(auto_eval_save_path, f'round_{round_num}')\n",
    "    \n",
    "    # Collect files from the round\n",
    "    json_files = collect_json_files(round_dir)\n",
    "\n",
    "    # Process the files in the round\n",
    "    all_results.extend(process_json_files(json_files))\n",
    "\n",
    "# Convert results to DataFrame for analysis\n",
    "df = pd.DataFrame(all_results)\n",
    "\n",
    "# Summarize total tokens per category for comparison\n",
    "summary = df.sum()\n",
    "\n",
    "# Create a token usage comparison DataFrame\n",
    "categories = [\"Question\", \"Human Answer\", \"Student Response\", \"Eval Response\", \"Score\",  \"Total\"]\n",
    "token_usage = [\n",
    "    summary[\"question_tokens\"],\n",
    "    summary[\"human_answer_tokens\"],\n",
    "    summary[\"model_answer_input_tokens\"],\n",
    "    summary[\"eval_response_tokens\"],\n",
    "    summary[\"score_tokens\"],\n",
    "    summary[\"total_tokens\"]\n",
    "]\n",
    "\n",
    "# Create a DataFrame for the results\n",
    "usage_df = pd.DataFrame({\n",
    "    \"Category\": categories,\n",
    "    \"Token Usage\": token_usage\n",
    "})\n",
    "\n",
    "# Save the token comparison table to a CSV file\n",
    "usage_csv_path = os.path.join(charts_dir, 'token_usage_comparison.csv')\n",
    "usage_df.to_csv(usage_csv_path, index=False)\n",
    "\n",
    "# Create a bar chart for token usage comparison\n",
    "x = np.arange(len(categories))\n",
    "\n",
    "# Plot Token Usage\n",
    "width = 0.35  # Width of the bars\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "bars = ax.bar(x, token_usage, width, label=\"Token Usage\", color=\"#4C72B0\")\n",
    "\n",
    "# Add values above the bars\n",
    "for bar in bars:\n",
    "    ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 5, f\"{int(bar.get_height())}\", ha=\"center\", fontsize=10)\n",
    "\n",
    "# Adjust the y-axis dynamically\n",
    "max_value = max(token_usage)\n",
    "ax.set_ylim(0, max_value * 1.2)  # Add 20% headroom above tallest bar\n",
    "\n",
    "# Add labels, title, and legend\n",
    "ax.set_ylabel(\"Token Count (Approx)\", fontsize=12)\n",
    "ax.set_title(\"Token Usage Comparison for Question-Answer Pairs\", fontsize=14)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(categories)\n",
    "ax.legend()\n",
    "\n",
    "# Save the chart as a PNG file\n",
    "chart_path = os.path.join(charts_dir, 'token_usage_comparison_chart.png')\n",
    "plt.savefig(chart_path, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
